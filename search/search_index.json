{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Genetic Programming over Spark cluster \u00b6 This work is part of my PhD research on extending Genetic Programming (GP) to learn from very large datasets. We used an existing GP implementation from the Evoltionary Algorithms library DEAP and we parallelized its execution over a Spark cluster. The code in Python is an application to Higgs boson classification problem with a dataset that has 11M instances. This code is available at hhmida/gp-spark and contains a jupyter notebook and a Python code file to be run on a Spark cluster. We detail the required steps to reproduce the experience in dockerized environment. Important note The given code and script files are crafted for a demo purpose. To run a real world code many tuning tasks must be carried out.","title":"About"},{"location":"#genetic-programming-over-spark-cluster","text":"This work is part of my PhD research on extending Genetic Programming (GP) to learn from very large datasets. We used an existing GP implementation from the Evoltionary Algorithms library DEAP and we parallelized its execution over a Spark cluster. The code in Python is an application to Higgs boson classification problem with a dataset that has 11M instances. This code is available at hhmida/gp-spark and contains a jupyter notebook and a Python code file to be run on a Spark cluster. We detail the required steps to reproduce the experience in dockerized environment. Important note The given code and script files are crafted for a demo purpose. To run a real world code many tuning tasks must be carried out.","title":" Genetic Programming over Spark cluster"},{"location":"environment/","text":"Preparing the environment \u00b6 In order to run the sample code we need to create a docker based cluster with the required software and configurations. The docker containers are adapted from rubenafo/docker-spark-cluster . You can read this article for more details about docker containers configuration. This environment consists of 4 nodes running in 4 seperate containers. Each node provides the following components: Scala, java and Python Hadoop and Spark A configured cluster orchestrated by YARN composed of 4 nodes: nodemaster (master node) node2 (slave) node3 (slave) node4 (slave) Docker images \u00b6 Two images are used. The first, called scalabase , is based on openjdk:8 and with required Python packages: numpy DEAP pyspark jupyter from openjdk:8 MAINTAINER Hmida Hmida <hhmida@gmail.com> USER root EXPOSE 22/tcp EXPOSE 22/udp RUN apt-get update && \\ apt-get install -y --no-install-recommends openssh-server vim WORKDIR /tmp RUN wget https://bootstrap.pypa.io/get-pip.py && python get-pip.py && pip install numpy deap jupyter pyspark RUN wget https://downloads.lightbend.com/scala/2.12.5/scala-2.12.5.tgz && \\ tar xzf scala-2.12.5.tgz && \\ mv scala-2.12.5 /usr/share/scala && \\ ln -s /usr/share/scala/bin/* /usr/bin && \\ rm scala-2.12.5.tgz && \\ \\ echo \"PubkeyAuthentication yes\" >> /etc/ssh/ssh_config && \\ echo \"Host *\" >> /etc/ssh/ssh_config CMD service ssh start && sleep infinity The second image sparkbase adds Hadoop and Spark and the requires configuration steps. FROM scalabase:latest MAINTAINER Hmida Hmida <hhmida@gmail.com> EXPOSE 8081 EXPOSE 8080 EXPOSE 8088 EXPOSE 7077 EXPOSE 9870 EXPOSE 4040 RUN useradd -m -s /bin/bash hadoop WORKDIR /home/hadoop USER hadoop RUN wget https://archive.apache.org/dist/hadoop/core/hadoop-3.2.0/hadoop-3.2.0.tar.gz RUN wget https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-without-hadoop.tgz RUN tar -zxf hadoop-3.2.0.tar.gz && \\ mv hadoop-3.2.0 hadoop && \\ tar -zxf spark-2.4.0-bin-without-hadoop.tgz && \\ mv spark-2.4.0-bin-without-hadoop spark && rm *gz RUN mkdir -p /home/hadoop/.ssh /home/hadoop/hadoop/logs \\ /home/hadoop/data/nameNode /home/hadoop/data/dataNode \\ /home/hadoop/data/namesecondary /home/hadoop/data/tmp && \\ touch /home/hadoop/hadoop/logs/fairscheduler-statedump.log # We don't care about the skeleton rc files, so overwrite COPY config/shellrc /home/hadoop/.bashrc COPY config/shellrc /home/hadoop/.profile COPY config/id_rsa* /home/hadoop/.ssh/ COPY config/id_rsa.pub /home/hadoop/.ssh/authorized_keys COPY config/workers /home/hadoop/spark/conf/slaves COPY config/sparkcmd.sh /home/hadoop/ COPY config/hadoop-env.sh /home/hadoop/ COPY config/core-site.xml config/hdfs-site.xml config/mapred-site.xml \\ config/yarn-site.xml config/workers /home/hadoop/hadoop/etc/hadoop/ USER hadoop RUN cat /home/hadoop/hadoop-env.sh >> /home/hadoop/hadoop/etc/hadoop/hadoop-env.sh USER root RUN chown -R hadoop /home/hadoop/.ssh /home/hadoop/.bashrc /home/hadoop/.profile \\ /home/hadoop/data /home/hadoop/hadoop-env.sh CMD service ssh start && sleep infinity Installation \u00b6 Clone the repository git clone https://github.com/hhmida/gp-spark Create the first docker image scalabase cd scalabase ./build.sh Create the second docker image sparkbase cd ../spark sudo ./build.sh Deploying the cluster cd .. ./cluster.sh deploy The source code will be available via docker volume option in the folder /opt/spark-apps The script will finish displaying the Hadoop and Spark admin URLs: Hadoop info @ nodemaster: http://172.18.1.1:8088/cluster Spark info @ nodemaster : http://172.18.1.1:8080/ DFS Health @ nodemaster : http://172.18.1.1:9870/dfshealth.html Jupyter notebook: http://172.18.1.1:8888/?tree URLs The IP address may differ from the example and changes upon redeploying and restarting the cluster. Options \u00b6 cluster.sh stop # Stop the cluster cluster.sh start # Start the cluster cluster.sh info # Shows handy URLs of running cluster cluster.sh deploy # Format the cluster and deploy images again Redeploying the cluster cluster.sh deploy # Format the cluster and deploy images again This will remove everything from HDFS. The content of /opt/spark-apps will not be changed. Putting data files on HDFS \u00b6 First, you need to connect to a node (here the nodemaster). docker exec -it -u hadoop nodemaster bash Then copying the file on Hadoop file system: cd /opt/spark-apps hadoop fs -put file Higgs Training and Test sets The Higgs dataset has 7 GB of data. The docker based cluster cannot process this amount of data. We put only 1/1000 of the real data. hadoop fs -put /opt/spark-apps/HIGGS_Training_Scaled_10500.csv hadoop fs -put /opt/spark-apps/HIGGS_Test_Scaled_500.csv","title":"Preparing the environment"},{"location":"environment/#preparing-the-environment","text":"In order to run the sample code we need to create a docker based cluster with the required software and configurations. The docker containers are adapted from rubenafo/docker-spark-cluster . You can read this article for more details about docker containers configuration. This environment consists of 4 nodes running in 4 seperate containers. Each node provides the following components: Scala, java and Python Hadoop and Spark A configured cluster orchestrated by YARN composed of 4 nodes: nodemaster (master node) node2 (slave) node3 (slave) node4 (slave)","title":" Preparing the environment"},{"location":"environment/#docker-images","text":"Two images are used. The first, called scalabase , is based on openjdk:8 and with required Python packages: numpy DEAP pyspark jupyter from openjdk:8 MAINTAINER Hmida Hmida <hhmida@gmail.com> USER root EXPOSE 22/tcp EXPOSE 22/udp RUN apt-get update && \\ apt-get install -y --no-install-recommends openssh-server vim WORKDIR /tmp RUN wget https://bootstrap.pypa.io/get-pip.py && python get-pip.py && pip install numpy deap jupyter pyspark RUN wget https://downloads.lightbend.com/scala/2.12.5/scala-2.12.5.tgz && \\ tar xzf scala-2.12.5.tgz && \\ mv scala-2.12.5 /usr/share/scala && \\ ln -s /usr/share/scala/bin/* /usr/bin && \\ rm scala-2.12.5.tgz && \\ \\ echo \"PubkeyAuthentication yes\" >> /etc/ssh/ssh_config && \\ echo \"Host *\" >> /etc/ssh/ssh_config CMD service ssh start && sleep infinity The second image sparkbase adds Hadoop and Spark and the requires configuration steps. FROM scalabase:latest MAINTAINER Hmida Hmida <hhmida@gmail.com> EXPOSE 8081 EXPOSE 8080 EXPOSE 8088 EXPOSE 7077 EXPOSE 9870 EXPOSE 4040 RUN useradd -m -s /bin/bash hadoop WORKDIR /home/hadoop USER hadoop RUN wget https://archive.apache.org/dist/hadoop/core/hadoop-3.2.0/hadoop-3.2.0.tar.gz RUN wget https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-without-hadoop.tgz RUN tar -zxf hadoop-3.2.0.tar.gz && \\ mv hadoop-3.2.0 hadoop && \\ tar -zxf spark-2.4.0-bin-without-hadoop.tgz && \\ mv spark-2.4.0-bin-without-hadoop spark && rm *gz RUN mkdir -p /home/hadoop/.ssh /home/hadoop/hadoop/logs \\ /home/hadoop/data/nameNode /home/hadoop/data/dataNode \\ /home/hadoop/data/namesecondary /home/hadoop/data/tmp && \\ touch /home/hadoop/hadoop/logs/fairscheduler-statedump.log # We don't care about the skeleton rc files, so overwrite COPY config/shellrc /home/hadoop/.bashrc COPY config/shellrc /home/hadoop/.profile COPY config/id_rsa* /home/hadoop/.ssh/ COPY config/id_rsa.pub /home/hadoop/.ssh/authorized_keys COPY config/workers /home/hadoop/spark/conf/slaves COPY config/sparkcmd.sh /home/hadoop/ COPY config/hadoop-env.sh /home/hadoop/ COPY config/core-site.xml config/hdfs-site.xml config/mapred-site.xml \\ config/yarn-site.xml config/workers /home/hadoop/hadoop/etc/hadoop/ USER hadoop RUN cat /home/hadoop/hadoop-env.sh >> /home/hadoop/hadoop/etc/hadoop/hadoop-env.sh USER root RUN chown -R hadoop /home/hadoop/.ssh /home/hadoop/.bashrc /home/hadoop/.profile \\ /home/hadoop/data /home/hadoop/hadoop-env.sh CMD service ssh start && sleep infinity","title":"Docker images"},{"location":"environment/#installation","text":"Clone the repository git clone https://github.com/hhmida/gp-spark Create the first docker image scalabase cd scalabase ./build.sh Create the second docker image sparkbase cd ../spark sudo ./build.sh Deploying the cluster cd .. ./cluster.sh deploy The source code will be available via docker volume option in the folder /opt/spark-apps The script will finish displaying the Hadoop and Spark admin URLs: Hadoop info @ nodemaster: http://172.18.1.1:8088/cluster Spark info @ nodemaster : http://172.18.1.1:8080/ DFS Health @ nodemaster : http://172.18.1.1:9870/dfshealth.html Jupyter notebook: http://172.18.1.1:8888/?tree URLs The IP address may differ from the example and changes upon redeploying and restarting the cluster.","title":"Installation"},{"location":"environment/#options","text":"cluster.sh stop # Stop the cluster cluster.sh start # Start the cluster cluster.sh info # Shows handy URLs of running cluster cluster.sh deploy # Format the cluster and deploy images again Redeploying the cluster cluster.sh deploy # Format the cluster and deploy images again This will remove everything from HDFS. The content of /opt/spark-apps will not be changed.","title":"Options"},{"location":"environment/#putting-data-files-on-hdfs","text":"First, you need to connect to a node (here the nodemaster). docker exec -it -u hadoop nodemaster bash Then copying the file on Hadoop file system: cd /opt/spark-apps hadoop fs -put file Higgs Training and Test sets The Higgs dataset has 7 GB of data. The docker based cluster cannot process this amount of data. We put only 1/1000 of the real data. hadoop fs -put /opt/spark-apps/HIGGS_Training_Scaled_10500.csv hadoop fs -put /opt/spark-apps/HIGGS_Test_Scaled_500.csv","title":"Putting data files on HDFS"},{"location":"notebook/","text":"Interactive execution with jupyter notebook \u00b6 This section describes how to run the jupyter notebook in /opt/spark-apps . All you need to do is to run: ./cluster.sh info and then open on the jupyter URL on your browser. The content of /opt/spark-apps will be displayed. Click on higgsSparkNotebook.ipynb to start the notebook. Higgs data Before running the example, training and test data must be put on HDFS as shown in the previous section. We did not deal with data preprocessing in this example. Logs are saved on higgs.log in spark-apps directory.","title":"Interactive execution with jupyter notebook"},{"location":"notebook/#interactive-execution-with-jupyter-notebook","text":"This section describes how to run the jupyter notebook in /opt/spark-apps . All you need to do is to run: ./cluster.sh info and then open on the jupyter URL on your browser. The content of /opt/spark-apps will be displayed. Click on higgsSparkNotebook.ipynb to start the notebook. Higgs data Before running the example, training and test data must be put on HDFS as shown in the previous section. We did not deal with data preprocessing in this example. Logs are saved on higgs.log in spark-apps directory.","title":"Interactive execution with jupyter notebook"},{"location":"submitting/","text":"Submitting to the cluster \u00b6 Configuration files mapred-site.xml and yarn-site.xml fix the amount of available memory. In this case each slave node has 1.5GB. Then the total available memory is 4.5GB (1.5 x 3). The number of core depends on your processor. The given example is tested on 8 core processor (8 x 3 = 24 cores). The code to be run on the cluster is in higgsSparkNRuns,py . It is the same code in the jupyter notebook with mininmal differences to allow executing many runs per submission. Submission is done by spark-submit script. In order to use the cluster and YARN as resource manager, we use the following options: --master yarn --deploy-mode cluster The script submit-GP.sh has a working configuration: #!/bin/bash if [ $# -eq 3 ] ; then spark-submit \\ --master yarn \\ --deploy-mode cluster \\ --executor-cores 2 \\ --num-executors 5 \\ --executor-memory 512m \\ --conf spark.yarn.executor.memoryOverhead = 30m \\ --conf spark.driver.memory = 512m \\ --conf spark.driver.cores = 3 \\ higgsSparkNRuns.py $1 $2 $3 exit else echo -e \"\\nUsage: submit-GP.sh nb_runs population_size nb_generations\\n\" fi To run this script: Connect to any node of the cluster with the user hadoop docker exec -it -u hadoop nodemaster bash Run the script cd /opt/spark-apps ./submit-GP.sh number_of_runs population_size number_of_generations Tuning Spark performance To reach an optimized Spark performance, a complex paramters' tuning phase must be conducted. Parameters like num-executors, executor-memory, executor-cores, ... need to be adjusted to meet the cluster available ressources. More details about this procedure can be found in this article from cloudera","title":"Submitting to the cluster"},{"location":"submitting/#submitting-to-the-cluster","text":"Configuration files mapred-site.xml and yarn-site.xml fix the amount of available memory. In this case each slave node has 1.5GB. Then the total available memory is 4.5GB (1.5 x 3). The number of core depends on your processor. The given example is tested on 8 core processor (8 x 3 = 24 cores). The code to be run on the cluster is in higgsSparkNRuns,py . It is the same code in the jupyter notebook with mininmal differences to allow executing many runs per submission. Submission is done by spark-submit script. In order to use the cluster and YARN as resource manager, we use the following options: --master yarn --deploy-mode cluster The script submit-GP.sh has a working configuration: #!/bin/bash if [ $# -eq 3 ] ; then spark-submit \\ --master yarn \\ --deploy-mode cluster \\ --executor-cores 2 \\ --num-executors 5 \\ --executor-memory 512m \\ --conf spark.yarn.executor.memoryOverhead = 30m \\ --conf spark.driver.memory = 512m \\ --conf spark.driver.cores = 3 \\ higgsSparkNRuns.py $1 $2 $3 exit else echo -e \"\\nUsage: submit-GP.sh nb_runs population_size nb_generations\\n\" fi To run this script: Connect to any node of the cluster with the user hadoop docker exec -it -u hadoop nodemaster bash Run the script cd /opt/spark-apps ./submit-GP.sh number_of_runs population_size number_of_generations Tuning Spark performance To reach an optimized Spark performance, a complex paramters' tuning phase must be conducted. Parameters like num-executors, executor-memory, executor-cores, ... need to be adjusted to meet the cluster available ressources. More details about this procedure can be found in this article from cloudera","title":"Submitting to the cluster"}]}